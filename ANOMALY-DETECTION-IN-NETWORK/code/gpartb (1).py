# -*- coding: utf-8 -*-
"""GpartB.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10SO9TvWEi2EAbMzzN68_iRjjLm--g-38
"""

# Imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, LabelEncoder
from sklearn.metrics import confusion_matrix, roc_auc_score, precision_score, recall_score, accuracy_score
from sklearn.neural_network import BernoulliRBM
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Dropout
from tensorflow.keras.optimizers import Adam

# Load CSV files
train_df = pd.read_csv('UNSW_NB15_training-set.csv')
test_df = pd.read_csv('UNSW_NB15_testing-set.csv')

# Combine for preprocessing
df = pd.concat([train_df, test_df], ignore_index=True)
df.drop(['id', 'attack_cat'], axis=1, inplace=True)

# Encode categorical columns
categorical_cols = df.select_dtypes(include='object').columns
for col in categorical_cols:
    df[col] = LabelEncoder().fit_transform(df[col])

# Split features and labels
X = df.drop('label', axis=1)
y = df['label']

# Normalize
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# Train/Val/Test Split
X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.3, random_state=42, stratify=y)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)

# Build Autoencoder Model
def build_autoencoder(input_dim, encoding_dim=64, dropout_rate=0.09):
    input_layer = Input(shape=(input_dim,))
    encoded = Dense(encoding_dim, activation="relu")(input_layer)
    dropout = Dropout(dropout_rate)(encoded)
    decoded = Dense(input_dim, activation="sigmoid")(dropout)
    autoencoder = Model(inputs=input_layer, outputs=decoded)
    return autoencoder

autoencoder = build_autoencoder(X_train.shape[1])
autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')

# Custom Training Loop with ROC-AUC Tracking
epochs = 20
batch_size = 64

train_loss = []
val_loss = []
val_auc_scores = []
acc_autoencoder = []  # List to track accuracy for Autoencoder

for epoch in range(epochs):
    # Train
    history = autoencoder.fit(X_train[y_train==0], X_train[y_train==0],
                              epochs=1,
                              batch_size=batch_size,
                              shuffle=True,
                              validation_data=(X_val[y_val==0], X_val[y_val==0]),
                              verbose=0)

    # Store losses
    train_loss.append(history.history['loss'][0])
    val_loss.append(history.history['val_loss'][0])

    # ROC-AUC on Validation
    reconstructions = autoencoder.predict(X_val, verbose=0)
    mse = np.mean(np.power(X_val - reconstructions, 2), axis=1)
    val_auc = roc_auc_score(y_val, mse)
    val_auc_scores.append(val_auc)

    # Accuracy for Autoencoder
    y_pred_ae = (mse > np.percentile(mse, 54)).astype(int)
    acc_autoencoder.append(accuracy_score(y_val, y_pred_ae))

    print(f"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss[-1]:.4f} - Val Loss: {val_loss[-1]:.4f} - Val ROC-AUC: {val_auc:.4f} - Val Accuracy: {acc_autoencoder[-1]:.4f}")

# Plot: Loss vs Epochs
plt.figure(figsize=(8,5))
plt.plot(range(1, epochs+1), train_loss, label='Train Loss')
plt.plot(range(1, epochs+1), val_loss, label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Autoencoder Loss vs Epochs')
plt.legend()
plt.grid()
plt.show()

# Plot: ROC-AUC vs Epochs
plt.figure(figsize=(8,5))
plt.plot(range(1, epochs+1), val_auc_scores, marker='o', color='green', label='Validation ROC-AUC')
plt.xlabel('Epochs')
plt.ylabel('ROC-AUC')
plt.title('Autoencoder Validation ROC-AUC vs Epochs')
plt.legend()
plt.grid()
plt.show()

# Plot: Accuracy vs Epochs (Autoencoder)
plt.figure(figsize=(8,5))
plt.plot(range(1, epochs+1), acc_autoencoder, marker='o', color='blue', label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Autoencoder Validation Accuracy vs Epochs')
plt.legend()
plt.grid()
plt.show()

# Final Autoencoder Evaluation on Test Set
reconstructions = autoencoder.predict(X_test, verbose=0)
mse = np.mean(np.power(X_test - reconstructions, 2), axis=1)
threshold = np.percentile(mse, 54)
y_pred_ae = (mse > threshold).astype(int)

# Metrics for Autoencoder
print("\nAutoencoder Evaluation:")
print("Accuracy:", accuracy_score(y_test, y_pred_ae))
print("Precision:", precision_score(y_test, y_pred_ae))
print("Recall:", recall_score(y_test, y_pred_ae))
print("ROC-AUC:", roc_auc_score(y_test, mse))

# Confusion Matrix Autoencoder
cm_ae = confusion_matrix(y_test, y_pred_ae)
plt.figure(figsize=(6,4))
sns.heatmap(cm_ae, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix: Autoencoder')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# ---- RBM + Logistic Regression ---- #
rbm = BernoulliRBM(n_components=32, learning_rate=0.001, batch_size=64, n_iter=30, random_state=42)
logistic = LogisticRegression(max_iter=1000)
classifier = Pipeline(steps=[('rbm', rbm), ('logistic', logistic)])
classifier.fit(X_train, y_train)

# Predict with RBM
y_pred_rbm = classifier.predict(X_test)

# Metrics for RBM
print("\nRBM Evaluation:")
print("Accuracy:", accuracy_score(y_test, y_pred_rbm))
print("Precision:", precision_score(y_test, y_pred_rbm))
print("Recall:", recall_score(y_test, y_pred_rbm))
print("ROC-AUC:", roc_auc_score(y_test, y_pred_rbm))

# Confusion Matrix RBM
cm_rbm = confusion_matrix(y_test, y_pred_rbm)
plt.figure(figsize=(6,4))
sns.heatmap(cm_rbm, annot=True, fmt='d', cmap='Greens')
plt.title('Confusion Matrix: RBM + Logistic Regression')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Final Accuracy Comparison
print("\nAccuracy Comparison between Autoencoder and RBM + Logistic Regression:")
print(f"Autoencoder Accuracy: {accuracy_score(y_test, y_pred_ae):.4f}")
print(f"RBM + Logistic Regression Accuracy: {accuracy_score(y_test, y_pred_rbm):.4f}")